{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import kstest\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forward_NeuralNet(nn.Module):\n",
    "    '''A neural network that only do feedforward.'''\n",
    "    def __init__(self, \n",
    "                layers: list[int],\n",
    "                activation: nn.Module=None,\n",
    "                weight_init: callable=None,\n",
    "                random_state=None) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if random_state is not None:\n",
    "            torch.manual_seed(random_state)\n",
    "        \n",
    "        self.layers = layers\n",
    "        self.activation = activation\n",
    "        self.weight_init = weight_init\n",
    "\n",
    "        # Construct the layers\n",
    "        Layers = []\n",
    "        inputs = layers[0]\n",
    "        for outputs in layers[1:-1]:\n",
    "            Layers.append(nn.Linear(inputs, outputs))\n",
    "            Layers.append(activation)\n",
    "            inputs = outputs\n",
    "\n",
    "        Layers.append(nn.Linear(layers[-2], layers[-1]))\n",
    "        self.network = nn.Sequential(*Layers)\n",
    "\n",
    "        # Inititalize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.network:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                self.weight_init(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, dataloader: torch.utils.data.DataLoader, device: torch.device) -> torch.Tensor:\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        outputs = []\n",
    "        with torch.inference_mode():\n",
    "            for X in dataloader:\n",
    "                X = X[0].to(device)\n",
    "                y = self.network(X)\n",
    "                outputs.append(y)\n",
    "        return torch.cat(outputs, dim=0) if outputs else torch.Tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stacked_one_hot(size, index, N):\n",
    "    # Create the one-hot vector\n",
    "    one_hot_vector = torch.zeros(size, dtype=torch.float)\n",
    "    one_hot_vector[index] = 1.0\n",
    "    # Stack the one-hot vector N times\n",
    "    stacked_tensor = one_hot_vector.repeat(N, 1)\n",
    "    return stacked_tensor\n",
    "\n",
    "generate_stacked_one_hot(3,1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [nn.init.xavier_uniform_, nn.init.xavier_normal_, nn.init.kaiming_uniform_, nn.init.kaiming_normal_, nn.init.orthogonal_]\n",
    "result = []\n",
    "N=10000\n",
    "for n_input in tqdm(range(2, 4)):\n",
    "    layers = []\n",
    "    layers.append(n_input)\n",
    "    for dist in ['uniform', 'normal']:\n",
    "        if dist == 'uniform':\n",
    "            X = torch.rand((N, n_input))\n",
    "        else:\n",
    "            X = torch.randn((N, n_input))\n",
    "        dataset = TensorDataset(X)\n",
    "        dataloader = DataLoader(dataset, batch_size=N)\n",
    "        for n_output in range(1, 4):\n",
    "            for weight in weights:\n",
    "                params = {'n_input': n_input, 'input_distribution':dist, 'n_output': n_output, 'weight': weight.__name__.rstrip('_')}\n",
    "                all_outputs = []\n",
    "                for i in range(N):\n",
    "                    model = Forward_NeuralNet([n_input, n_output], weight_init=weight)\n",
    "                    output = model.forward(dataloader, 'mps').to('cpu').numpy()\n",
    "                    all_outputs.append(output)\n",
    "                all_outputs = np.concatenate(all_outputs, axis=0)\n",
    "                result.append((output, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(np.random.randn(10000))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result:\n",
    "    print(i[1])\n",
    "    for j in range(i[0].shape[1]): \n",
    "        fig = px.histogram(i[0].T[j], histnorm='probability density')\n",
    "        fig.update_layout(showlegend=False, title=f'output_{j}')\n",
    "        fig.show()\n",
    "        print(f'mean: {np.mean(i[0].T[j])}, median: {np.median(i[0].T[j])}, variance: {np.var(i[0].T[j])}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
